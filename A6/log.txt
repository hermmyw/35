log.txt
1. Run the command sort --version to make sure you're using a new-enough version. 
$ sort --version
sort (GNU coreutils) 8.29
Copyright (C) 2017 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <https://gnu.org/licenses/gpl.html>.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

Written by Mike Haertel and Paul Eggert.


2. Generate a file containing 10,000,000 random single-precision floating point numbers, in textual form, one per line with no white space. 
	$ od -An -f -N 40000000 < /dev/urandom | tr -s ' ' '\n' > random.txt
		Generate a file with 10000000 floating point numbers. Since each float is 4 bytes, the file needs to be 40000000 bytes. Transform all spaces to new lines.
	$ cat random.txt | wc -l
	10000001
		Check the number of lines. The output should be 10000000.
	$ cat random.txt | sed '/^[[:space:]]*$/d' > myFile.txt
		Remove the additional new line.

3. time -p to time the command sort -g on that data, with the output sent to /dev/null. 

4. Invoke sort with the --parallel option as well as the -g option, and run your benchmark with 1, 2, 4, and 8 threads, in each case recording the real, user, and system time. 

	$ echo $PATH
	/usr/local/cs/bin/:/usr/lib64/qt-3.3/bin:/u/cs/ugrad/huimin/perl5/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/X11R6/bin:/usr/local/cs/bin:/u/cs/ugrad/huimin/bin
	$ time -p sort -g myFile.txt > /dev/null
	real 21.91
	user 114.87
	sys 0.49
	$ time -p sort -g --parallel=1 myFile.txt > /dev/null
	real 109.57
	user 109.27
	sys 0.29
	$ time -p sort -g --parallel=2 myFile.txt > /dev/null
	real 60.27
	user 114.60
	sys 0.34
	$ time -p sort -g --parallel=4 myFile.txt > /dev/null
	real 33.71
	user 113.41
	sys 0.36
	$ time -p sort -g --parallel=8 myFile.txt > /dev/null
	real 21.89
	user 114.54
	sys 0.45
	
The real time gets shorter as the number of threads increases due to parallelism.
I don't see an obvious changing pattern with respect to system time, and user time
almost stays constant. 
When we can divide tasks into various independent steps with shared memory by running
multiple cores together, we can get a better performance, as long as the overhead is
still small.

